import AVFoundation
import CoreMedia

final class NDIAudioFrame: @unchecked Sendable {
	let receiver: NDIReceiver
	fileprivate var ref: NDIlib_audio_frame_v3_t

	init(_ ref: NDIlib_audio_frame_v3_t, receiver: NDIReceiver) {
		self.ref = ref
		self.receiver = receiver
	}

	deinit {
		NDIlib_recv_free_audio_v3(receiver.pNDI_recv, &ref)
	}

	/// The sample-rate of this buffer.
	var sampleRate: Int {
		.init(ref.sample_rate)
	}

	/// The number of audio channels.
	var numberOfChannels: Int {
		.init(ref.no_channels)
	}

	/// The number of audio samples per channel.
	var numberOfSamples: Int {
		.init(ref.no_samples)
	}

	/// The timecode of this frame.
	var timecode: Duration {
		Duration.nanoseconds(ref.timecode * 100)
	}

	/// Per frame metadata for this frame. This is a NULL terminated UTF8 string that should be in XML format.
	/// If you do not want any metadata then you may specify NULL here.
	var metadata: String? {
		guard let p_metadata = ref.p_metadata else { return nil }
		return String(cString: p_metadata)
	}

	/// This is only valid when receiving a frame and is specified as the time that was the exact
	/// moment that the frame was submitted by the sending side and is generated by the SDK. If this value is
	/// NDIlib_recv_timestamp_undefined then this value is not available and is NDIlib_recv_timestamp_undefined.
	var timestamp: Date? {
		Date(ndiTimestamp: ref.timestamp)
	}

	var dataByteSize: Int {
		.init(ref.no_channels * ref.channel_stride_in_bytes)
	}

	var duration: Duration {
		.nanoseconds((Int64(ref.no_samples) * 1_000_000_000) / Int64(ref.sample_rate))
	}

	var sampleBuffer: CMSampleBuffer? {
		switch ref.FourCC {
		case NDIlib_FourCC_audio_type_FLTP:
			var context = CFAllocatorContext(
				version: 0,
				info: Unmanaged.passRetained(self).toOpaque(),
				retain: nil,
				release: nil,
				copyDescription: nil,
				allocate: nil,
				reallocate: nil,
				deallocate: { _, info in
					guard let info else { return }
					Unmanaged<NDIAudioFrame>.fromOpaque(info).release()
				},
				preferredSize: nil
			)
			let blockAllocator = CFAllocatorCreate(kCFAllocatorDefault, &context)
				.takeUnretainedValue()

			let dataByteSize = ref.no_channels * ref.channel_stride_in_bytes

			var blockBuffer: CMBlockBuffer?
			let status = CMBlockBufferCreateWithMemoryBlock(
				allocator: kCFAllocatorDefault,
				memoryBlock: ref.p_data,
				blockLength: Int(dataByteSize),
				blockAllocator: blockAllocator,
				customBlockSource: nil,
				offsetToData: 0,
				dataLength: Int(dataByteSize),
				flags: 0,
				blockBufferOut: &blockBuffer
			)

			guard status == kCMBlockBufferNoErr, let blockBuffer else {
				return nil
			}

			var sampleBuffer: CMSampleBuffer?
			let sampleSizeArray: [Int] = [MemoryLayout<Float32>.size]

			var outputDescription = AudioStreamBasicDescription(
				mSampleRate: .init(sampleRate),
				mFormatID: kAudioFormatLinearPCM,
				mFormatFlags: kAudioFormatFlagIsFloat | kAudioFormatFlagIsNonInterleaved,
				mBytesPerPacket: 4, // 32-bit (4 bytes) per sample, per channel
				mFramesPerPacket: 1,
				mBytesPerFrame: 4, // 32-bit (4 bytes) per sample, per channel
				mChannelsPerFrame: .init(numberOfChannels),
				mBitsPerChannel: 32,
				mReserved: 0
			)
			var formatDescription: CMAudioFormatDescription?
			let cmAudioFormatDescriptionCreateStatus = CMAudioFormatDescriptionCreate(
				allocator: nil,
				asbd: &outputDescription,
				layoutSize: 0,
				layout: nil,
				magicCookieSize: 0,
				magicCookie: nil,
				extensions: nil,
				formatDescriptionOut: &formatDescription
			)
			guard cmAudioFormatDescriptionCreateStatus == noErr else {
				print("Failed to create CMAudioFormatDescription: \(status)")
				return nil
			}

			let sampleBufferStatus = CMSampleBufferCreateReady(
				allocator: kCFAllocatorDefault,
				dataBuffer: blockBuffer,
				formatDescription: formatDescription,
				sampleCount: .init(ref.no_samples),
				sampleTimingEntryCount: 0,
				sampleTimingArray: nil,
				sampleSizeEntryCount: 1,
				sampleSizeArray: sampleSizeArray,
				sampleBufferOut: &sampleBuffer
			)

			guard let sampleBuffer, sampleBufferStatus == noErr else {
				return nil
			}

			let presentationTime = CMTime(value: .init(ref.timestamp), timescale: .init(NDI.timescale))
			CMSampleBufferSetOutputPresentationTimeStamp(sampleBuffer, newValue: presentationTime)

			return sampleBuffer
		default:
			return nil
		}
	}
}

extension NDIAudioFrame: CustomStringConvertible {
	var description: String {
		"<NDIAudioFrame sample_rate: \(sampleRate), no_channels: \(numberOfChannels), no_samples: \(numberOfSamples), timecode: \(timecode), channel_stride_in_bytes: \(ref.channel_stride_in_bytes), p_metadata: \(metadata ?? ""), timestamp: \(timestamp?.formatted() ?? "none")>"
	}
}
